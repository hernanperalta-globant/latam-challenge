{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "\n",
    "The problem entails extracting insights from a large JSON file, such as top dates with the most tweets, most used emojis, and most influential users. The challenge lies in efficiently processing this large dataset, and 2 main concerns arise: optimizing memory usage vs. optimizing execution time.\n",
    "Taking this into account, we can inspect the problem from these 2 points of view.\n",
    "\n",
    "## Execution time\n",
    "\n",
    "Here, the question of CPU-bound vs. I/O bound arises. Let's analyze these aspects in the problem:\n",
    "\n",
    "- CPU-bound tasks: to find the requested insights, some necessary operations on tweets will be: sort, search by key, compare, parse attributes, regex matching, merging control data structures such as counters, keeping track of them, etc.\n",
    "- I/O tasks: the major task of this type is reading the file, and although it can be considered not-so-small, the data processing does not require writing back to disk, file merging or splitting, transferring through network or similar.\n",
    "\n",
    "Hence, we can state that this is a **data processing** problem, for which we choose a **CPU-bound** solution to this: Python's `multiprocessing` module. This module allows a developer to spawn processes using high-level abstractions, and also by using processes we can bypass the GIL as each process has its own Python interpreter (a well-known Python issue). This leaves behind other approaches such as `async.io` or `concurrent.futures`.\n",
    "\n",
    "Using this module we can divide the file tweets into tweet batches, have each worker process its batch, and merge each result. Of course, some operations can be parallelized (e.g.: creating control structures from a tweet) and some cannot (e.g.: sorting). This is similar to what is done in MapReduce engines.\n",
    "\n",
    "## Memory usage\n",
    "\n",
    "The main bottleneck of this problem is handling a 398MBs file without creating leaks, that could come from inefficient handling of the file, such as reading the entire file into memory, storing unnecessary data structures, traversing it too many times, not keeping correct track of data points , etc.\n",
    "\n",
    "To overcome this, we can take advantage of several Python built-in capacities:\n",
    "\n",
    "- Streaming file data: generators allow us to read the file in chunks and process each data point on-demand, i.e.: we can process tweets as we need them instead of loading everything into memory.\n",
    "- Lazy evaluation: `yield` lets us create code that will be evaluated only when we ask for it.\n",
    "- Efficient data structures: the solution heavily relies on `collections.Counter`, a `dict` subclass where elements are keys and counts are values, similar to a bag where there are no duplicates, but also keeping track of the count. This is memory-efficient because operations on it are O(1) and does not save elements that are absent or duplicate.\n",
    "\n",
    "### Summary\n",
    "\n",
    "For the memory optimized functions (`q*_memory()`) the approach is streaming the file into memory, processing each tweet as it comes, create the control structures needed for the calculation and, when the stream is ended, finally release it and perform calculation.\n",
    "For the time optimized functions (`q*_time()`) the approach is splitting tweets into batches, assign each batch to a worker, perform all parallelizable  operations on them (_map_) and then merge the results and perform everything operation that's been left out (_reduce_).\n",
    "\n",
    "## Possible improvements\n",
    "\n",
    "- In-place sorting for smaller memory footprint (e.g.: with effectful `list.sort()`).\n",
    "- Stream batches in each worker process instead of loading everything into memory and then splitting the work.\n",
    "- Partitioning or indexing data for more efficient reading.\n",
    "\n",
    "## Side notes\n",
    "\n",
    "- Why didn't I use cloud?: for this challenge in particular, I felt that developing raw algorithms and using nothing but as pure Python as I could represented better what the esence is of optimizing for memory vs. execution time. In general, all data libraries and frameworks end up on similar conclusions: for optimizing memory we need **reading data as-needed** and **efficient data structures**, for optimizing time we need **parallelizable operations** and **horizontal scaling**. Nevertheless, the spirit of the solution could be taken to a PySpark job in GCP Dataproc or AWS EMR for time optimization, or to AWS Lambda for memory optimization, where you could set the memory limit thinking about what it takes to process 1 stream of data instead of the whole dataset. This type of data could be easily taken to S3, and for more capacities to a document db or even BigQuery, where you could also use SQL for insights.\n",
    "\n",
    "- Solutions are not completely isolated, time optimized code can use things that are also used in memory optimized code an viceversa.\n",
    "\n",
    "- Assumptions:\n",
    "    - For `q3_*`, users mentioned in quoted or retweeted tweets are not taken into account, it's assumed that all mentions in quotes or retweets eventually appear in the document as their own tweet, for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "from src.q1.q1_memory import q1_memory\n",
    "from src.q1.q1_time import q1_time\n",
    "from src.q2.q2_memory import q2_memory\n",
    "from src.q2.q2_time import q2_time\n",
    "from src.q3.q3_memory import q3_memory\n",
    "from src.q3.q3_time import q3_time\n",
    "\n",
    "file_path = 'farmers-protest-tweets-2021-2-4.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_time_result = q1_time(file_path)\n",
    "q1_memory_result = q1_memory(file_path)\n",
    "\n",
    "print(f'q1_time     : {q1_time_result}')\n",
    "print(f'q1_memory   : {q1_memory_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q1_time q1_time(file_path)\n",
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path)\n",
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_time_result = q2_time(file_path)\n",
    "q2_memory_result = q2_memory(file_path)\n",
    "\n",
    "print(f'q2_time     : {q2_time_result}')\n",
    "print(f'q2_memory   : {q2_memory_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q2_time q2_time(file_path)\n",
    "%memit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q2_memory q2_memory(file_path)\n",
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_time_result = q3_time(file_path)\n",
    "q3_memory_result = q3_memory(file_path)\n",
    "\n",
    "print(f'q3_time     : {q3_time_result}')\n",
    "print(f'q3_memory   : {q3_memory_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s cumulative -l 10 q3_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q3_time q3_time(file_path)\n",
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q3_memory q3_memory(file_path)\n",
    "%memit q3_memory(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
